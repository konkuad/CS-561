{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pmf of the multinomial distribution $(X_1,X_2,\\dots,X_K)\\sim\\text{Multi}(\\mathbf{p})$ with multinomial parameter $\\mathbf{p}=(p_1,p_2,\\dots,p_k)$ is given as\n",
    "$$\n",
    "P(X_i=k_i:i=1,2,\\dots,K)=\\frac{n!}{k_1!k_2!\\dots k_K!} p_1^{k_1} p_2^{k_2} \\dots p_K^{k_K}\n",
    "$$\n",
    "We can derive the KL-divergence between $\\text{Multi}(\\mathbf{p})$ and $\\text{Multi}(\\mathbf{q})$ as follows:\n",
    "\\begin{align*}\n",
    "    D_{KL}(\\text{Multi}(\\mathbf{p})||\\text{Multi}(\\mathbf{q})) \n",
    "    &= \\sum_{k_1+k_2+\\dots+k_K=n} \\frac{n!}{k_1!k_2!\\dots k_K!} p_1^{k_1} p_2^{k_2} \\dots p_K^{k_K} \\log \\left( \\frac{p_1^{k_1} p_2^{k_2} \\dots p_K^{k_K}}{q_1^{k_1} q_2^{k_2} \\dots q_K^{k_K}} \\right) \\\\\n",
    "    &= \\sum_{k_1+k_2+\\dots+k_K=n} \\frac{n!}{k_1!k_2!\\dots k_K!} p_1^{k_1} p_2^{k_2} \\dots p_K^{k_K} \\sum_{i=1}^K k_i \\log \\left( \\frac{p_i}{q_i} \\right) \\\\\n",
    "    &= \\sum_{i=1}^K \\log \\left( \\frac{p_i}{q_i} \\right) \\sum_{k_1+k_2+\\dots+k_K=n} k_i \\frac{n!}{k_1!k_2!\\dots k_K!} p_1^{k_1} p_2^{k_2} \\dots p_K^{k_K}   \\\\\n",
    "    &= \\sum_{i=1}^K \\log \\left( \\frac{p_i}{q_i} \\right) \\mathbb{E}[X_i:(X_1,X_2,\\dots,X_K)\\sim\\text{Multi}(\\mathbf{p})] \\\\\n",
    "    &= \\sum_{i=1}^K \\log \\left( \\frac{p_i}{q_i} \\right) np_i \\\\\n",
    "    &= n \\sum_{i=1}^K p_i \\log \\left( \\frac{p_i}{q_i} \\right) \\\\\n",
    "    &= n D_{KL}(\\mathbf{p}||\\mathbf{q})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, solver=&#x27;saga&#x27;, tol=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, solver=&#x27;saga&#x27;, tol=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', solver='saga', tol=0.01)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(np.shape(x_train))\n",
    "\n",
    "def vectorize(_image):\n",
    "    return np.reshape(_image, (-1,1))\n",
    "\n",
    "vec_x_train = np.squeeze(np.array([vectorize(m) for m in x_train]))\n",
    "vec_x_test = np.squeeze(np.array([vectorize(m) for m in x_test]))\n",
    "\n",
    "# generate an instance of the logistic regression class model with multinomial logistic regression\n",
    "model = LogisticRegression(solver='saga', tol=0.01, multi_class='multinomial')\n",
    "model.fit(vec_x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97      5972\n",
      "           1       0.98      0.97      0.97      6822\n",
      "           2       0.92      0.94      0.93      5856\n",
      "           3       0.91      0.92      0.92      6099\n",
      "           4       0.94      0.94      0.94      5853\n",
      "           5       0.89      0.91      0.90      5283\n",
      "           6       0.97      0.96      0.96      5994\n",
      "           7       0.94      0.95      0.95      6199\n",
      "           8       0.91      0.90      0.90      5896\n",
      "           9       0.92      0.91      0.92      6026\n",
      "\n",
      "    accuracy                           0.94     60000\n",
      "   macro avg       0.94      0.94      0.94     60000\n",
      "weighted avg       0.94      0.94      0.94     60000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.97      1010\n",
      "           1       0.98      0.96      0.97      1161\n",
      "           2       0.90      0.93      0.91       995\n",
      "           3       0.91      0.90      0.91      1024\n",
      "           4       0.93      0.93      0.93       983\n",
      "           5       0.86      0.91      0.89       850\n",
      "           6       0.95      0.95      0.95       963\n",
      "           7       0.92      0.93      0.93      1021\n",
      "           8       0.88      0.87      0.88       985\n",
      "           9       0.91      0.91      0.91      1008\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### compute the accuracy and print a classification report\n",
    "y_train_hat = model.predict(vec_x_train)\n",
    "y_test_hat = model.predict(vec_x_test)\n",
    "print(\"Train\")\n",
    "print(classification_report(y_train_hat, y_train))\n",
    "print(\"Test\")\n",
    "print(classification_report(y_test_hat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# convert the 10 classes to one hot encoding\n",
    "one_hot = OneHotEncoder()\n",
    "Y_train = one_hot.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "Y_test = one_hot.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "print(np.shape(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 9.9260 - accuracy: 0.8382\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 6.0376 - accuracy: 0.8794\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.7018 - accuracy: 0.8839\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.5749 - accuracy: 0.8853\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.3354 - accuracy: 0.8875\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.3640 - accuracy: 0.8880\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.2025 - accuracy: 0.8895\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.1955 - accuracy: 0.8895\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.1506 - accuracy: 0.8888\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 5.1100 - accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(vec_x_train, Y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 2s 1ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "Train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      6118\n",
      "           1       0.94      0.99      0.96      6423\n",
      "           2       0.88      0.94      0.91      5538\n",
      "           3       0.89      0.89      0.89      6093\n",
      "           4       0.93      0.86      0.89      6367\n",
      "           5       0.85      0.87      0.86      5273\n",
      "           6       0.96      0.93      0.95      6117\n",
      "           7       0.95      0.88      0.92      6752\n",
      "           8       0.90      0.82      0.86      6439\n",
      "           9       0.75      0.91      0.82      4880\n",
      "\n",
      "    accuracy                           0.90     60000\n",
      "   macro avg       0.90      0.90      0.90     60000\n",
      "weighted avg       0.91      0.90      0.90     60000\n",
      "\n",
      "Test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96      1014\n",
      "           1       0.95      0.98      0.97      1104\n",
      "           2       0.86      0.94      0.90       945\n",
      "           3       0.88      0.89      0.88       996\n",
      "           4       0.92      0.84      0.88      1076\n",
      "           5       0.83      0.86      0.84       859\n",
      "           6       0.94      0.92      0.93       989\n",
      "           7       0.94      0.87      0.90      1105\n",
      "           8       0.90      0.80      0.84      1101\n",
      "           9       0.74      0.91      0.82       811\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_train_hat = model.predict(vec_x_train)\n",
    "y_train_hat = Y_train_hat.argmax(-1)\n",
    "Y_test_hat = model.predict(vec_x_test)\n",
    "y_test_hat = Y_test_hat.argmax(-1)\n",
    "print(\"Train\")\n",
    "print(classification_report(y_train_hat, y_train))\n",
    "print(\"Test\")\n",
    "print(classification_report(y_test_hat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.c) The neural network model is an interative variant to the logistic regression model. Therefore, the neural network takes faster to train, but performs worse overall due to the logistic regression model fitting the parameter with the whole dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
